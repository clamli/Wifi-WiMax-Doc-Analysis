import scrapy

import xlwt
class MentorSpider(scrapy.Spider):
    name = "mentor"
    allowed_domians = ["mentor.ieee.org"];
    start_urls = ["https://mentor.ieee.org/802.11/documents"]

#    def start_requests(self)




    def parse(self, response):
        # //basic version
        path = "F:\WireLessNLPGRA\wenjian.csv"
        mainInfo = response.xpath("//td/text()").extract()
        num = int(len(mainInfo) / 6);
        timeInfo = response.xpath('//*[contains(@class, "b_data_row")]//td/div/text()').extract()
        download = response.xpath('//*[@class="list_actions"]/a/@href').extract();

        wb = xlwt.Workbook()
        sheet = wb.add_sheet("page1_80211")

        for i in range(num):
            sheet.write(i, 0, timeInfo[i*2])
            sheet.write(i, 1, mainInfo[i*6])
            sheet.write(i, 2, mainInfo[i*6 + 1])
            sheet.write(i, 3, mainInfo[i*6 + 2])
            sheet.write(i, 4, mainInfo[i*6 + 3])
            sheet.write(i, 5, mainInfo[i*6 + 4])
            sheet.write(i, 6, mainInfo[i*6 + 5])
            sheet.write(i, 7, timeInfo[i*2 + 1])
            sheet.write(i, 8, download[i])
        wb.save(path)
        # print("successfully write in " + str(num) + " rows")
#        yield Request(url)

        # for i in range(num):
        #     yield {"created time": timeInfo[i*2],
        #          "year": mainInfo[i*6],
        #          "DCN": mainInfo[i*6 + 1],
        #          "Rev": mainInfo[i*6 + 2],
        #          "Group": mainInfo[i*6 + 3],
        #          "Title": mainInfo[i*6 + 4],
        #          "Author": mainInfo[i*6 + 5],
        #          "uplodated time": timeInfo[i*2 + 1],
        #          "dowload": download[i]}
